{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data with replacing it with mode\n",
    "def replace_missing_val(census_data):\n",
    "    census_data['workclass'].replace(np.NaN, census_data['workclass'].mode()[0] , inplace=True )\n",
    "    census_data['occupation'].replace(np.NaN, census_data['occupation'].mode()[0] , inplace=True )\n",
    "    census_data['native-country'].replace(np.NaN, census_data['native-country'].mode()[0] , inplace=True )\n",
    "    return census_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin width for continuous attributes\n",
    "bin_width_dict = {}\n",
    "bin_width_dict['age'] = 5\n",
    "bin_width_dict['fnlwgt'] = 23000\n",
    "bin_width_dict['education-num'] = 2\n",
    "bin_width_dict['capital-loss'] = 300\n",
    "bin_width_dict['capital-gain'] = 6000\n",
    "bin_width_dict['hours-per-week'] = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to create bins\n",
    "def binning(column ,binningwidth, min_range, max_range):\n",
    "    if min_range <= column <= max_range: \n",
    "        return column - (column%binningwidth)\n",
    "    elif (column < min_range):\n",
    "        return (min_range -(min_range%binningwidth) - binningwidth)\n",
    "    elif (colum > max_range):\n",
    "        return max_range + (binningwidth+(max_range%binningwidth))\n",
    "\n",
    "    \n",
    "def binningDF(dataframe):\n",
    "    dataframe.loc[:,'age'] = dataframe.age.apply(binning, args=(bin_width_dict['age'], dataframe['age'].min(), dataframe['age'].max(),))\n",
    "    dataframe.loc[:,'fnlwgt'] = dataframe.fnlwgt.apply(binning, args=(bin_width_dict['fnlwgt'], dataframe['fnlwgt'].min(), dataframe['fnlwgt'].max(),))\n",
    "    dataframe.loc[:,'education-num'] = dataframe['education-num'].apply(binning, args=(bin_width_dict['education-num'], dataframe['education-num'].min(), dataframe['education-num'].max(),))\n",
    "    dataframe.loc[:,'capital-loss'] = dataframe['capital-loss'].apply(binning, args=(bin_width_dict['capital-loss'], dataframe['capital-loss'].min(), dataframe['capital-loss'].max(),))\n",
    "    dataframe.loc[:,'capital-gain'] = dataframe['capital-gain'].apply(binning, args=(bin_width_dict['capital-gain'], dataframe['capital-gain'].min(), dataframe['capital-gain'].max(), ))\n",
    "    dataframe.loc[:,'hours-per-week'] = dataframe['hours-per-week'].apply(binning, args=(bin_width_dict['hours-per-week'], dataframe['hours-per-week'].min(), dataframe['hours-per-week'].max(),)) \n",
    "    return dataframe    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using equi-width binning \n",
    "def NaiveBayesian_fit(training_set): \n",
    "    #apply binning    \n",
    "    binned_df = binningDF(training_set)  \n",
    "    #dictinary to save weights\n",
    "    weightdict ={}     \n",
    "    # income >50K --> yes\n",
    "    # income <=50K --> no\n",
    "    yes_df = binned_df.loc[binned_df['class'] == ' >50K']\n",
    "    no_df = binned_df.loc[binned_df['class'] == ' <=50K']\n",
    "    \n",
    "    totalYeslables = len(yes_df) # count of records in >50K class\n",
    "    totalNolables = len(no_df) # count of records in <= 50K class\n",
    "    binned_df = binned_df.drop(labels= 'class', axis=1)\n",
    "    # iterate over each column to calculate feature weights\n",
    "    for label, content in binned_df.iteritems(): \n",
    "        if label not in weightdict.keys():\n",
    "            weightdict[label]={}\n",
    "            for val in content: # add all unique value of the attribute to dictionary key\n",
    "                if val not in weightdict[label].keys():\n",
    "                    weightdict[label][val] = {} \n",
    "                    #store only count of records in both class for feature\n",
    "                    weightdict[label][val]['yes'] = sum(y for y in yes_df[label] == val )\n",
    "                    weightdict[label][val]['no'] = sum(n for n in no_df[label] == val)\n",
    "        \n",
    "    weightdict['total_yes_lable'] = totalYeslables\n",
    "    weightdict['total_no_lable'] = totalNolables\n",
    "    return weightdict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict using equi-width binning\n",
    "def NaiveBayesian_predict(test_dataset, weightdict):\n",
    "    predicted_lable = []\n",
    "    test_dataset = binningDF(test_dataset)\n",
    "    \n",
    "    total_yes_targets = weightdict['total_yes_lable']\n",
    "    total_no_targets = weightdict['total_no_lable']\n",
    "    # iterate rows of the data frame to calculate probabily of incoming records\n",
    "    for index, content in test_dataset.iterrows():\n",
    "        yes_prob = []\n",
    "        no_prob = []\n",
    "        for lable in content.keys():\n",
    "            if content[lable] in weightdict[lable].keys():\n",
    "                #calculate the probability using weights stored in dictionary\n",
    "                if (weightdict[lable][content[lable]]['yes'] and weightdict[lable][content[lable]]['no']):\n",
    "                    yes_prob.append(weightdict[lable][content[lable]]['yes'] /total_yes_targets)\n",
    "                    no_prob.append(weightdict[lable][content[lable]]['no'] /total_no_targets)\n",
    "                else:\n",
    "                    #apply laplacian correction for feature present in only one of the class\n",
    "                    yes_prob.append((weightdict[lable][content[lable]]['yes']+1) /(total_yes_targets+len(weightdict[lable])))\n",
    "                    no_prob.append((weightdict[lable][content[lable]]['no'] +1)/(total_no_targets+len(weightdict[lable])))\n",
    "                \n",
    "            else:\n",
    "                #apply laplacian correction for feature not present in training set\n",
    "                yes_prob.append(1/total_yes_targets + len(weightdict[lable]))\n",
    "                no_prob.append(1/total_no_targets + len(weightdict[lable]))\n",
    "                \n",
    "        predicted_yes = np.prod(np.array(yes_prob)) * (total_yes_targets/(total_yes_targets+total_no_targets))\n",
    "        predicted_no = np.prod(np.array(no_prob)) * (total_no_targets/(total_yes_targets+total_no_targets))\n",
    "        \n",
    "        \n",
    "        if predicted_yes >= predicted_no:\n",
    "            predicted_lable.append(' >50K')\n",
    "        else:\n",
    "            predicted_lable.append(' <=50K')\n",
    "                \n",
    "    return predicted_lable           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to perform k-fold cross validation\n",
    "def k_fold_cv(data, k):\n",
    "    accuracy_list = []\n",
    "    F1_score = []\n",
    "    MCC_list = []   \n",
    "   \n",
    "    split_ratio = k/100\n",
    "    sub_data = []    \n",
    "    # divide data into k sub-dataframes    \n",
    "    for i in range(0, k):\n",
    "        sub_data.append(data.sample(frac= split_ratio))      \n",
    "                \n",
    "    for i in range(0,k): # perform k - fold\n",
    "        test_split = sub_data[i] # assign test data in each k-iterations\n",
    "        train_split = pd.DataFrame() \n",
    "        for j in range(0, k): # sub-dataframes other than test data are used to train the model\n",
    "            if (j != i):   \n",
    "                train_split = pd.concat([train_split,sub_data[j]])        \n",
    "        \n",
    "        test_data_label = test_split['class']\n",
    "        test_split = test_split.drop(labels= 'class', axis=1)\n",
    "        # train the model to calculate the weights\n",
    "        weightdict = NaiveBayesian_fit(train_split)\n",
    "        # predict the labels for test data\n",
    "        predicted_val = NaiveBayesian_predict(test_split,weightdict)\n",
    "        \n",
    "        true_pos = len([i for i, j in zip(test_data_label,predicted_val) if i == j and j == ' >50K'])\n",
    "        true_neg = len([i for i, j in zip(test_data_label,predicted_val) if i == j and j == ' <=50K'])\n",
    "        false_pos = len([i for i, j in zip(test_data_label,predicted_val) if i != j and j == ' >50K'])\n",
    "        false_neg = len([i for i, j in zip(test_data_label,predicted_val) if i != j and j == ' <=50K'])\n",
    "        precision = true_pos/(true_pos+false_pos)\n",
    "        recall = true_pos/(true_pos+false_neg)\n",
    "                \n",
    "        accuracy = len([i for i, j in zip(test_data_label,predicted_val) if i == j])/len(test_data_label)\n",
    "        F1_score.append((2*precision*recall)/(precision+recall)) \n",
    "        #calculate matthews correlation coefficient \n",
    "        Mcc_denominator = math.sqrt((true_pos+false_pos) * (true_pos+false_neg)*(true_neg+false_neg)*(false_pos+true_neg))\n",
    "        MCC = ((true_pos*true_neg) - (false_pos*false_neg))/Mcc_denominator\n",
    "        MCC_list.append(MCC)\n",
    "        \n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "    return (accuracy_list, F1_score, MCC_list ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayesian_model(data, k, remove_missing_value):    \n",
    "    if remove_missing_value:\n",
    "        data.dropna(inplace=True)\n",
    "        print(\"Removed missing values from dataset\")\n",
    "    else:\n",
    "        data = replace_missing_val(data)\n",
    "        print(\"Replaced missing values from dataset with mode of the attributes\")\n",
    "    \n",
    "    accuracy_list, F1_score, MCC_list = k_fold_cv(data, k)\n",
    "    print(\"avg accuracy of the model :\", sum(accuracy_list)/len(accuracy_list))\n",
    "    print(\"F1-measure is :\", sum(F1_score)/len(F1_score))\n",
    "    print(\"Matthews corelation coefficient :\", sum(MCC_list)/len(MCC_list))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data using pandas\n",
    "header = ['age', 'workclass','fnlwgt', 'education','education-num','marital-status','occupation','relationship','race','sex',\n",
    "          'capital-gain','capital-loss','hours-per-week','native-country','class']         \n",
    "census_data = pd.read_table('adult.data',sep=',', header=None)\n",
    "census_data.columns = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataset  (32561, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of dataset \", census_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "details of missing values in attributes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1836\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education-num        0\n",
       "marital-status       0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital-gain         0\n",
       "capital-loss         0\n",
       "hours-per-week       0\n",
       "native-country     583\n",
       "class                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the number of missing values\n",
    "census_data.replace(' ?', np.NaN, inplace=True)    \n",
    "print(\"details of missing values in attributes\") \n",
    "census_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Dataset after replacing values \\n\")\n",
    "# census_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed missing values from dataset\n",
      "avg accuracy of the model : 0.823076923076923\n",
      "F1-measure is : 0.6856098824682734\n",
      "Matthews corelation coefficient : 0.5723776955982555\n"
     ]
    }
   ],
   "source": [
    "# parameters for model are data_set, value of K for K fold cv and remove_missing_value is true or false \n",
    "\n",
    "# Model with removed missing values records\n",
    "Naive_Bayesian_model(census_data, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced missing values from dataset with mode of the attributes\n",
      "avg accuracy of the model : 0.8242705570291777\n",
      "F1-measure is : 0.6858895055205045\n",
      "Matthews corelation coefficient : 0.5720853191506535\n"
     ]
    }
   ],
   "source": [
    "#Model with replaced missing values\n",
    "Naive_Bayesian_model(census_data, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
